import langextract as lx
from langextract.data import ExampleData, Extraction
from langextract.providers.openai import OpenAILanguageModel
import textwrap
import time

# 1. å®šä¹‰æå–è§„åˆ™ - ä¸­æ–‡æ”¿åºœé‡‡è´­ä¿¡æ¯
prompt = textwrap.dedent("""\
    ä»æ”¿åºœé‡‡è´­å…¬å‘Šä¸­æå–å…³é”®ä¿¡æ¯ã€‚æ¯ä¸ªå­—æ®µç±»å‹åªæå–ä¸€æ¬¡ï¼Œè¿”å›æœ€æƒå¨ã€æœ€å®Œæ•´çš„å€¼ã€‚
    
    **ä¸¥æ ¼çº¦æŸï¼š**
    1. extraction_text å¿…é¡»æ˜¯åŸæ–‡çš„ç²¾ç¡®æ–‡æœ¬ç‰‡æ®µï¼Œé€å­—é€å¥å¤åˆ¶
    2. **ç¦æ­¢é‡å¤æå–**ï¼šæ¯ä¸ª extraction_class åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­åªèƒ½å‡ºç°ä¸€æ¬¡
    3. å¦‚æœåŒä¸€ä¿¡æ¯å‡ºç°å¤šæ¬¡ï¼Œä¼˜å…ˆæå–æ ‡æ³¨ä¸º"å…¬å‘Šä¿¡æ¯"æˆ–è¡¨æ ¼å¼€å¤´çš„æƒå¨æ•°æ®
    4. ä¸è¦æå–å­—æ®µæ ‡ç­¾ï¼ˆå¦‚"é‡‡è´­é¡¹ç›®åç§° |"ï¼‰ã€è¯´æ˜æ–‡å­—æˆ–é¡µé¢å¯¼èˆªå†…å®¹
    5. å¦‚æœæŸå­—æ®µåœ¨æ–‡æ¡£ä¸­ä¸å­˜åœ¨ï¼Œå®Œå…¨è·³è¿‡è¯¥å­—æ®µ
    6. ä¸è¦è¿”å›æˆ‘æä¾›çš„æ¡ˆä¾‹ä¸­çš„ä»»ä½•ä¿¡æ¯ï¼Œæå–æ—¶åªå…³æ³¨è¾“å…¥æ–‡æœ¬å†…å®¹ã€‚
    
    æå–ä»¥ä¸‹9ä¸ªå­—æ®µï¼ˆå„æå–ä¸€æ¬¡ï¼‰ï¼š
    - å…¬å‘Šæ—¶é—´ï¼šå…¬å‘Šå‘å¸ƒæ—¥æœŸæ—¶é—´ï¼ˆYYYYå¹´MMæœˆDDæ—¥ HH:MMæ ¼å¼ï¼‰ï¼Œä¼˜å…ˆæå–"å…¬å‘Šæ—¶é—´ |"åçš„å®Œæ•´æ—¶é—´
    - é¡¹ç›®åç§°ï¼šå®Œæ•´çš„é‡‡è´­é¡¹ç›®åç§°ï¼ˆä¼˜å…ˆä»"é‡‡è´­é¡¹ç›®åç§° |"æˆ–"äºŒã€é¡¹ç›®åç§°ï¼š"æå–ï¼‰
    - é‡‡è´­å•ä½åç§°ï¼šå‘èµ·é‡‡è´­çš„æœºæ„å…¨ç§°ï¼ˆä¼˜å…ˆä»"é‡‡è´­å•ä½ |"æå–ï¼‰
    - é‡‡è´­å•ä½åœ°å€ï¼šé‡‡è´­å•ä½å®Œæ•´åœ°å€ï¼ˆä¼˜å…ˆä»"é‡‡è´­å•ä½åœ°å€ |"æå–ï¼Œéœ€åŒ…å«é—¨ç‰Œå·ï¼‰
    - ä¾›åº”å•†åç§°ï¼šæ’åç¬¬ä¸€çš„ä¸­æ ‡ä¾›åº”å•†å…¨ç§°ï¼ˆä»"ä¸‰ã€é‡‡è´­ç»“æœ"çš„è¡¨æ ¼ç¬¬ä¸€è¡Œæå–ï¼‰
    - ä¾›åº”å•†åœ°å€ï¼šä¸­æ ‡ä¾›åº”å•†å®Œæ•´åœ°å€ï¼ˆä»é‡‡è´­ç»“æœè¡¨æ ¼ä¸­å¯¹åº”è¡Œæå–ï¼‰
    - ä¸­æ ‡é‡‘é¢ï¼šæ€»ä¸­æ ‡é‡‘é¢ï¼ˆä»"æ€»ä¸­æ ‡é‡‘é¢ |"æå–ï¼Œéœ€åŒ…å«è´§å¸ç¬¦å·å’Œå•ä½ï¼‰
    - é‡‡è´­ç±»åˆ«ï¼šå“ç›®ç±»åˆ«ï¼ˆå¦‚"æœåŠ¡ç±»"ã€"è´§ç‰©ç±»"ï¼Œä»ä¸»è¦æ ‡çš„ä¿¡æ¯éƒ¨åˆ†æå–ï¼‰
    - é‡‡è´­æ ‡çš„ï¼šå…·ä½“é‡‡è´­å†…å®¹ï¼ˆä»"å“ç›®åç§°"åˆ—æå–ï¼Œå¦‚"å†œç•œäº§å“æ‰¹å‘æœåŠ¡"ï¼‰
    """)

# 2. æä¾›é«˜è´¨é‡çš„ä¸­æ–‡ç¤ºä¾‹ - ä½¿ç”¨ä¸åŒç±»å‹çš„é‡‡è´­æ¡ˆä¾‹ï¼ˆè®¾å¤‡é‡‡è´­ï¼‰
examples = [
    ExampleData(
        text="""å…¬å‘Šä¿¡æ¯ï¼š
é‡‡è´­é¡¹ç›®åç§°ï¼šå¹¿å·å¸‚è·¯ç¯æ™ºèƒ½ç›‘æ§ç³»ç»Ÿé‡‡è´­é¡¹ç›®
å“ç›®ï¼šç…§æ˜è®¾å¤‡
é‡‡è´­å•ä½ï¼šå¹¿å·å¸‚åŸå¸‚ç®¡ç†å±€
è¡Œæ”¿åŒºåŸŸï¼šå¤©æ²³åŒº | å…¬å‘Šæ—¶é—´ï¼š2024å¹´01æœˆ15æ—¥ 10:30
æ€»ä¸­æ ‡é‡‘é¢ï¼šï¿¥368.50 ä¸‡å…ƒï¼ˆäººæ°‘å¸ï¼‰

è”ç³»äººåŠè”ç³»æ–¹å¼ï¼š
é¡¹ç›®è”ç³»äººï¼šæå·¥
é‡‡è´­å•ä½ï¼šå¹¿å·å¸‚åŸå¸‚ç®¡ç†å±€
é‡‡è´­å•ä½åœ°å€ï¼šå¹¿å·å¸‚å¤©æ²³åŒºå¤©æ²³è·¯123å·
ä»£ç†æœºæ„åç§°ï¼šå¹¿ä¸œé‡‡è´­ä»£ç†æœ‰é™å…¬å¸

ä¸‰ã€é‡‡è´­ç»“æœ
åˆåŒåŒ…1(æ™ºèƒ½è·¯ç¯ç›‘æ§ç³»ç»Ÿ)ï¼š
ä¾›åº”å•†åç§° | ä¾›åº”å•†åœ°å€ | ä¸­æ ‡é‡‘é¢
æ·±åœ³æ™ºæ…§ç…§æ˜ç§‘æŠ€æœ‰é™å…¬å¸ | æ·±åœ³å¸‚å—å±±åŒºç§‘æŠ€å›­å—åŒºæ·±å—å¤§é“9988å· | ï¿¥368.50 ä¸‡å…ƒ

å››ã€ä¸»è¦æ ‡çš„ä¿¡æ¯
åˆåŒåŒ…1(æ™ºèƒ½è·¯ç¯ç›‘æ§ç³»ç»Ÿ)ï¼š
æœåŠ¡ç±»
å“ç›®å· | å“ç›®åç§° | é‡‡è´­æ ‡çš„
1-1 | ç…§æ˜è®¾å¤‡ | æ™ºèƒ½è·¯ç¯ç›‘æ§ç³»ç»Ÿ

å‘å¸ƒæ—¥æœŸï¼š2024å¹´01æœˆ15æ—¥ 10:30""",
        extractions=[
            Extraction(
                extraction_class="å…¬å‘Šæ—¶é—´",
                extraction_text="2024å¹´01æœˆ15æ—¥ 10:30",
                attributes={"æ ¼å¼": "æ—¥æœŸ+æ—¶é—´"}
            ),
            Extraction(
                extraction_class="é¡¹ç›®åç§°",
                extraction_text="å¹¿å·å¸‚è·¯ç¯æ™ºèƒ½ç›‘æ§ç³»ç»Ÿé‡‡è´­é¡¹ç›®",
                attributes={"ç±»å‹": "è´§ç‰©é‡‡è´­"}
            ),
            Extraction(
                extraction_class="é‡‡è´­å•ä½åç§°",
                extraction_text="å¹¿å·å¸‚åŸå¸‚ç®¡ç†å±€",
                attributes={"æ€§è´¨": "æ”¿åºœéƒ¨é—¨"}
            ),
            Extraction(
                extraction_class="é‡‡è´­å•ä½åœ°å€",
                extraction_text="å¹¿å·å¸‚å¤©æ²³åŒºå¤©æ²³è·¯123å·",
                attributes={"åŒºåŸŸ": "å¤©æ²³åŒº"}
            ),
            Extraction(
                extraction_class="ä¾›åº”å•†åç§°",
                extraction_text="æ·±åœ³æ™ºæ…§ç…§æ˜ç§‘æŠ€æœ‰é™å…¬å¸",
                attributes={"è§’è‰²": "ä¸­æ ‡ä¾›åº”å•†"}
            ),
            Extraction(
                extraction_class="ä¾›åº”å•†åœ°å€",
                extraction_text="æ·±åœ³å¸‚å—å±±åŒºç§‘æŠ€å›­å—åŒºæ·±å—å¤§é“9988å·",
                attributes={"åŒºåŸŸ": "å—å±±åŒº"}
            ),
            Extraction(
                extraction_class="ä¸­æ ‡é‡‘é¢",
                extraction_text="ï¿¥368.50 ä¸‡å…ƒï¼ˆäººæ°‘å¸ï¼‰",
                attributes={"æ•°å€¼": "3685000.00", "å•ä½": "ä¸‡å…ƒ"}
            ),
            Extraction(
                extraction_class="é‡‡è´­ç±»åˆ«",
                extraction_text="æœåŠ¡ç±»",
                attributes={"å“ç›®": "ç…§æ˜è®¾å¤‡"}
            ),
            Extraction(
                extraction_class="é‡‡è´­æ ‡çš„",
                extraction_text="æ™ºèƒ½è·¯ç¯ç›‘æ§ç³»ç»Ÿ",
                attributes={"ç±»å‹": "ç›‘æ§è®¾å¤‡"}
            ),
        ]
    )
]

# 3. è¯»å–çœŸå®çš„æµ‹è¯•æ–‡æœ¬
with open("test_output.txt", "r", encoding="utf-8") as f:
    input_text = f.read()

# 4. é…ç½® DeepSeek æ¨¡å‹
MODEL_ID = "deepseek-chat"
API_KEY = "sk-2895a83fa10c49eeb262f6c5139ad423"
BASE_URL = "https://api.deepseek.com"
MODEL_TEMPERATURE = 0.1  # Low temperature keeps output stable
MODEL_FORMAT = lx.data.FormatType.JSON

def configure_model():
    """Return the DeepSeek-backed extraction model."""
    return OpenAILanguageModel(
        model_id=MODEL_ID,
        api_key=API_KEY,
        base_url=BASE_URL,
        temperature=MODEL_TEMPERATURE,
        format_type=MODEL_FORMAT,
    )


def deduplicate_extractions(document):
    """å»é‡ï¼šæ¯ä¸ª extraction_class åªä¿ç•™ä¸€ä¸ªæœ€ä¼˜ç»“æœ"""
    if not hasattr(document, 'extractions') or not document.extractions:
        return 0
    
    seen_classes = {}
    deduplicated = []
    
    for ext in document.extractions:
        class_name = ext.extraction_class
        
        # å¦‚æœè¿™ä¸ªç±»åˆ«è¿˜æ²¡è§è¿‡ï¼Œç›´æ¥æ·»åŠ 
        if class_name not in seen_classes:
            seen_classes[class_name] = ext
            deduplicated.append(ext)
        else:
            # å¦‚æœå·²ç»å­˜åœ¨ï¼Œæ¯”è¾ƒä¼˜å…ˆçº§ï¼ˆæœ‰ char_interval ä¸”æ›´é•¿çš„ä¼˜å…ˆï¼‰
            existing = seen_classes[class_name]
            
            # ä¼˜å…ˆé€‰æ‹©æœ‰æ˜ç¡®ä½ç½®ä¿¡æ¯çš„
            if ext.char_interval is not None and existing.char_interval is None:
                # æ›¿æ¢ä¸ºæ›´å¥½çš„ç‰ˆæœ¬
                deduplicated.remove(existing)
                deduplicated.append(ext)
                seen_classes[class_name] = ext
            elif ext.char_interval is not None and existing.char_interval is not None:
                # éƒ½æœ‰ä½ç½®ä¿¡æ¯ï¼Œé€‰æ‹©æ–‡æœ¬æ›´é•¿çš„ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
                if len(ext.extraction_text) > len(existing.extraction_text):
                    deduplicated.remove(existing)
                    deduplicated.append(ext)
                    seen_classes[class_name] = ext
    
    removed_count = len(document.extractions) - len(deduplicated)
    document.extractions = deduplicated
    return removed_count


custom_model = configure_model()

# è®°å½•å¼€å§‹æ—¶é—´
start_time = time.time()

# 5. æ‰§è¡Œæå–
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
    debug=True,
)

# è®°å½•ç»“æŸæ—¶é—´
end_time = time.time()
elapsed_time = end_time - start_time

# åå¤„ç†ï¼šå»é‡
removed_duplicates = deduplicate_extractions(result)
if removed_duplicates > 0:
    print(f"\nğŸ”§ å»é‡å¤„ç†ï¼šç§»é™¤äº† {removed_duplicates} æ¡é‡å¤æå–")

# 6. ä¿å­˜ç»“æœ
lx.io.save_annotated_documents(
    [result], 
    output_name="extraction_results_chinese.jsonl", 
    output_dir="."
)

# 7. ç”Ÿæˆå¯è§†åŒ–
html_content = lx.visualize("extraction_results_chinese.jsonl")
with open("visualization_chinese.html", "w", encoding="utf-8") as f:
    if hasattr(html_content, 'data'):
        f.write(html_content.data)
    else:
        f.write(html_content)

# 8. æ‰“å°ç»“æœ
print("\nâœ… æå–å®Œæˆ!")
print("="*60)
print(f"â±ï¸  ç”¨æ—¶: {elapsed_time:.2f} ç§’")
print(f"ğŸ“„ æå–ç»“æœå·²ä¿å­˜: extraction_results_chinese.jsonl")
print(f"ğŸ¨ å¯è§†åŒ–æ–‡ä»¶å·²ç”Ÿæˆ: visualization_chinese.html")
print("\nğŸ” æå–çš„å†…å®¹:")
print(result)

# 9. è§£æå¹¶ç¾åŒ–è¾“å‡º
if hasattr(result, 'extractions') and result.extractions:
    print("\n" + "="*60)
    print("ğŸ“Š æå–è¯¦æƒ…:")
    print("="*60)
    for i, ext in enumerate(result.extractions, 1):
        print(f"\n{i}. {ext.extraction_class}")
        print(f"   æ–‡æœ¬: {ext.extraction_text}")
        print(f"   å±æ€§: {ext.attributes}")
